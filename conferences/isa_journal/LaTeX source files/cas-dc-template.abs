Word embedding models typically learn a vector representation for each word, based on the co-occurrence statistics in the contexts across a text corpus. On the other hand, topic modeling techniques produce a distribution over words for each latent topic and a distribution over latent topics for each document in a given corpus. We combine word embedding models with topic models for a better semantic representation which in turn used to represent documents. To demonstrate the value of our approach we choose document categorization as the mainstream NLP task. We compare the classification performance of well-known  machine learning classifiers on the proposed document vectors and traditional Doc2vec approach. We show that the proposed document vectors; combination of Word2vec and LDA improves the classification results and reduce the complexity of the resulting model.
	
